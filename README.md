ğŸ“– Knowledge Distillation fÃ¼r Textklassifikation
ğŸš€ Knowledge Distillation fÃ¼r effiziente Textklassifikation:
Dieses Projekt nutzt Knowledge Distillation, um ein leistungsfÃ¤higes Teacher-Modell zu trainieren und daraus ein kompaktes Student-Modell abzuleiten. Das Student-Modell ist fÃ¼r den Einsatz auf Edge-GerÃ¤ten und fÃ¼r ressourcenarme Umgebungen optimiert.

ğŸ”¹ Features
âœ… Knowledge Distillation â€“ Effiziente Modellkomprimierung
âœ… Sentiment-Analyse fÃ¼r Textklassifikation â€“ Positiv/Negativ-Vorhersage
âœ… Modularer Machine Learning Workflow â€“ Datenaufbereitung, Training, Evaluierung
âœ… REST API mit Flask â€“ Echtzeit-Modelleinsatz fÃ¼r Vorhersagen
âœ… GPU-UnterstÃ¼tzung â€“ Automatische Nutzung von CUDA falls verfÃ¼gbar

ğŸ— Projektstruktur

knowledge_distillation_text/
â”œâ”€â”€ README.md                          # Projektbeschreibung, Anleitung und Nutzungshinweise
â”œâ”€â”€ requirements.txt                   # Liste der benÃ¶tigten Python-Pakete (Dependencies)
â”œâ”€â”€ setup.py                           # Setup-Skript zur Installation des Projekts als Python-Paket
â”œâ”€â”€ .gitignore                         # Dateien/Ordner, die Git ignorieren soll
â”œâ”€â”€ Dockerfile                         # Docker-Konfiguration fÃ¼r Container-Deployment
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ default.yaml                   # Standard-Konfiguration (Hyperparameter, Pfade, etc.)
â”‚   â””â”€â”€ logging.conf                   # Logging-Konfiguration
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ vocab.json                     # Vokabular-Datei (Mapping von Tokens zu Indizes)
â”‚   â”œâ”€â”€ sample_dataset.csv             # Beispieldatensatz fÃ¼r Sentiment-Analyse
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ knowledge_distillation_text_documentation.json  # AusfÃ¼hrliche Projektdokumentation
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py                    
â”‚   â”œâ”€â”€ data_preprocessing.py          # Tokenisierung, Vokabularaufbau, Padding
â”‚   â”œâ”€â”€ models.py                      # Definitionen von Teacher- und Student-Modellen
â”‚   â”œâ”€â”€ training.py                    # Trainingsfunktionen fÃ¼r Teacher und Student
â”‚   â”œâ”€â”€ evaluation.py                  # Evaluierungsmetriken (Accuracy, Precision, etc.)
â”‚   â”œâ”€â”€ utils.py                       # Hilfsfunktionen (Logging, Modellmanagement)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_preprocessing.py     # Unit-Tests fÃ¼r Datenverarbeitung
â”‚   â”œâ”€â”€ test_models.py                 # Tests fÃ¼r Modellarchitekturen
â”‚   â”œâ”€â”€ test_training.py               # Tests fÃ¼r Trainings- und Evaluierungsfunktionen
â”œâ”€â”€ distillNLP_main.py # Hauptskript fÃ¼r Training, Evaluierung & Distillation
â””â”€â”€ app.py                             # Flask-API fÃ¼r Echtzeit-Inferenz
ğŸš€ Installation
1ï¸âƒ£ Klonen des Repositorys

git clone https://github.com/dein-benutzername/knowledge_distillation_text.git
cd knowledge_distillation_text
2ï¸âƒ£ Erstellen einer virtuellen Umgebung

python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
3ï¸âƒ£ Installieren der AbhÃ¤ngigkeiten

pip install -r requirements.txt
ğŸ‹ Modelltraining & Evaluierung
1ï¸âƒ£ Training des Teacher-Modells

python distillation_edge_text_extended.py --mode teacher
ğŸ”¹ Speichert das Teacher-Modell unter models/teacher_model.pth.

2ï¸âƒ£ Training des Student-Modells mit Knowledge Distillation

python distillation_edge_text_extended.py --mode student
ğŸ”¹ Speichert das Student-Modell unter models/student_model.pth.

3ï¸âƒ£ Evaluierung des Student-Modells

python distillation_edge_text_extended.py --mode evaluate
ğŸ”¹ Zeigt Genauigkeit, Precision, Recall und F1-Score.

ğŸŒ Bereitstellung der API
1ï¸âƒ£ Starten der Flask-API

python app.py
ğŸ”¹ Die API lÃ¤uft unter http://localhost:5000.

2ï¸âƒ£ Testen der API mit cURL

curl -X POST "http://localhost:5000/predict" -H "Content-Type: application/json" -d '{"text": "I love this movie"}'
ğŸ”¹ Antwort:

  "text": "I love this movie",
  "prediction": 1,
  "sentiment": "positiv"
}
ğŸ³ Docker Deployment
1ï¸âƒ£ Erstellen des Docker-Containers

docker build -t knowledge_distillation_text .
2ï¸âƒ£ Starten des Containers

docker run -p 5000:5000 knowledge_distillation_text
ğŸ“Š Evaluierungsmethoden
Nach dem Training und der Distillation werden folgende Metriken zur Modellbewertung berechnet:

Metrik	Beschreibung
Accuracy	Gibt an, wie viele Vorhersagen korrekt sind.
Precision	Misst, wie viele als positiv vorhergesagte Klassen tatsÃ¤chlich positiv sind.
Recall	Zeigt, wie viele der tatsÃ¤chlichen positiven Klassen korrekt erkannt wurden.
F1-Score	Harmonic Mean zwischen Precision & Recall.
Confusion Matrix	Zeigt die Verteilung der Vorhersagen Ã¼ber die Klassen.
ğŸ”¥ Erweiterungen & Optimierungen
ğŸ“Œ Multilinguale UnterstÃ¼tzung â€“ Modelle fÃ¼r verschiedene Sprachen trainieren
ğŸ“Œ Optimierte Modelle fÃ¼r Mobile & Edge-Devices â€“ Quantisierte Student-Modelle fÃ¼r IoT
ğŸ“Œ Automatische Hyperparameter-Suche â€“ Grid Search fÃ¼r die besten Modellparameter

ğŸ“œ Lizenz
Dieses Projekt steht unter der MIT-Lizenz.

ğŸ¤ Mitwirken
BeitrÃ¤ge sind willkommen! Falls du dieses Projekt verbessern mÃ¶chtest:

Forke das Repository ğŸš€
Erstelle einen neuen Branch (feature/neues-feature)
Committe deine Ã„nderungen
Erstelle einen Pull-Request ğŸ’¡
ğŸ“© Kontakt & Support
ğŸ‘¤ Dein Name
ğŸ“§ dein.email@example.com
ğŸŒ GitHub-Projektseite

ğŸ“Œ Fazit
âœ… End-to-End Machine Learning Pipeline â€“ Von Training bis API-Deployment
âœ… Knowledge Distillation fÃ¼r optimierte Modelle â€“ Schnelle & kompakte Student-Modelle
âœ… Echtzeit-API fÃ¼r Textklassifikation â€“ Sentiment-Analyse in Sekundenschnelle
âœ… Skalierbar & erweiterbar fÃ¼r Deep Learning â€“ Perfekt fÃ¼r Edge- & Cloud-Deployment ğŸš€

Falls du weitere Fragen hast oder Anpassungen benÃ¶tigst, lass es mich wissen! ğŸ¯ğŸš€

Hier ist der vollstÃ¤ndige Inhalt der requirements.txt-Datei, die alle benÃ¶tigten Python-AbhÃ¤ngigkeiten fÃ¼r dein Knowledge Distillation fÃ¼r Textklassifikation-Projekt enthÃ¤lt.

ğŸ“„ requirements.txt
txt
Kopieren
Bearbeiten
# PyTorch und verwandte Pakete fÃ¼r Deep Learning
torch>=1.10.0
torchvision>=0.11.0
torchaudio>=0.10.0

# NLP-Bibliotheken fÃ¼r Textverarbeitung
nltk>=3.6.5
spacy>=3.2.0
transformers>=4.15.0
sentencepiece>=0.1.96

# Datenverarbeitung und Speicherung
numpy>=1.21.2
scipy>=1.7.1
pandas>=1.3.3
pyyaml>=5.4.1

# Modellbewertung und Visualisierung
scikit-learn>=0.24.2
matplotlib>=3.4.3
seaborn>=0.11.2

# Web-API fÃ¼r Inferenz
flask>=2.0.2
flask-restful>=0.3.9

# Logging und Debugging
loguru>=0.5.3

# Tests und QualitÃ¤tssicherung
pytest>=6.2.5
pytest-cov>=2.12.1

# Optimierung & Scheduling
tqdm>=4.62.3
ğŸ“Œ ErklÃ¤rung der AbhÃ¤ngigkeiten
Kategorie	Pakete	Beschreibung
Deep Learning	torch, torchvision, torchaudio	PyTorch-Framework fÃ¼r das Training von neuronalen Netzwerken.
NLP & Textverarbeitung	nltk, spacy, transformers, sentencepiece	Tokenisierung, Embeddings, vortrainierte Modelle (z.â€¯B. BERT, DistilBERT).
Datenverarbeitung	numpy, scipy, pandas, pyyaml	Verarbeitung von Textdaten und Speicherung von Konfigurationen.
Modellbewertung & Visualisierung	scikit-learn, matplotlib, seaborn	Berechnung von Metriken (Accuracy, F1-Score) und Visualisierung der Ergebnisse.
Web-API & Flask	flask, flask-restful	Bereitstellung eines REST-API-Servers fÃ¼r Modellvorhersagen.
Logging & Debugging	loguru	Erweiterte Logging-FunktionalitÃ¤ten fÃ¼r einfaches Debugging.
Tests & QualitÃ¤tssicherung	pytest, pytest-cov	Automatische Tests fÃ¼r Modelltraining, Inferenz & Datenverarbeitung.
Optimierung & Training	tqdm	Fortschrittsbalken fÃ¼r lange Trainingsprozesse.
ğŸ“Œ Installation der AbhÃ¤ngigkeiten
Nach dem Klonen des Repositorys kannst du die Pakete mit folgendem Befehl installieren:

pip install -r requirements.txt
Falls du eine virtuelle Umgebung verwenden mÃ¶chtest:

python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
pip install -r requirements.txt
ğŸ“Œ Warum diese requirements.txt?
âœ… Minimal & Optimiert â€“ EnthÃ¤lt nur essenzielle Pakete fÃ¼r Training, Inferenz & API.
âœ… Modular & Skalierbar â€“ UnterstÃ¼tzt sowohl Deep Learning als auch klassische NLP-Techniken.
âœ… Einfach zu installieren â€“ Keine unnÃ¶tigen Bibliotheken oder redundante Pakete.

Falls du weitere Pakete fÃ¼r GPU-Support (z.â€¯B. CUDA), Model Deployment oder Hyperparameter-Tuning hinzufÃ¼gen mÃ¶chtest, lass es mich wissen! ğŸš€

Warum verwenden wir Soft Labels vom Teacher fÃ¼r das Student-Modell?
Die Ãœbertragung von Soft Labels (weicheren Wahrscheinlichkeitsverteilungen) vom Teacher-Modell auf das Student-Modell ist der Kern von Knowledge Distillation. Diese Methode wurde von Geoffrey Hinton et al. eingefÃ¼hrt und hilft dabei, kleinere Modelle zu trainieren, die sich Ã¤hnlich wie das grÃ¶ÃŸere Modell verhalten, aber effizienter sind.

1ï¸âƒ£ Was sind Soft Labels?
Anstatt dass das Teacher-Modell nur eine harte 1/0-Klassifikation ausgibt, gibt es eine Wahrscheinlichkeitsverteilung Ã¼ber alle Klassen zurÃ¼ck.
Beispiel fÃ¼r harte Labels (klassische Cross-Entropy):

{"Positive": 1, "Negative": 0}
Beispiel fÃ¼r Soft Labels (Teacher-Modell mit Temperatur=2):

{"Positive": 0.85, "Negative": 0.15}
â†’ Das Modell ist sich sicher, aber nicht zu 100 %.
ğŸ’¡ Soft Labels enthalten mehr Informationen darÃ¼ber, wie das Teacher-Modell entscheidet!

2ï¸âƒ£ Warum ist das sinnvoll fÃ¼r das Student-Modell?
1ï¸âƒ£ Verhindert Overfitting an harte Labels

Wenn das Student-Modell nur harte Labels sieht, lernt es weniger Ã¼ber feine Unterschiede zwischen Klassen.
Soft Labels zeigen an, wie stark ein Beispiel zu einer Klasse gehÃ¶rt, nicht nur ob es richtig oder falsch ist.
2ï¸âƒ£ FÃ¼hrt zu besseren Generalisierungen

Klassische harte Labels geben keine Ã„hnlichkeitsinformationen weiter.
Beispiel:
"Great movie" â†’ 100 % Positiv
"Nice film" â†’ 85 % Positiv, 15 % Neutral
"Not bad" â†’ 55 % Positiv, 45 % Negativ
Das Student-Modell lernt so, dass manche SÃ¤tze ambivalent sind und nicht immer klar einer Kategorie zugeordnet werden kÃ¶nnen.
3ï¸âƒ£ Komprimiert Wissen in ein kleineres Modell

Das Teacher-Modell ist groÃŸ & komplex (z.â€¯B. BERT, LSTMs).
Das Student-Modell ist kompakt & schneller.
Das Student-Modell kann durch Soft Labels viel von der Intelligenz des Teacher-Modells Ã¼bernehmen, ohne dessen GrÃ¶ÃŸe zu haben.
4ï¸âƒ£ Bessere Performance auf Edge-GerÃ¤ten & Mobile AI

Viele Unternehmen (Google, OpenAI) setzen Knowledge Distillation ein, um kleinere Modelle zu trainieren, die fast so gut sind wie groÃŸe Modelle, aber weniger Rechenleistung benÃ¶tigen.
3ï¸âƒ£ Beispiel: Klassische vs. Distillation-Training
Ohne Knowledge Distillation (nur harte Labels)
Das Student-Modell sieht nur 0/1-Klassen:

["Great movie!" â†’ 1]
["Terrible acting!" â†’ 0]
ğŸ”´ Problem:

Das Modell lernt, dass eine Vorhersage immer 100 % sicher ist.
Keine Information darÃ¼ber, wie Ã¤hnlich zwei SÃ¤tze sind.
Mit Knowledge Distillation (Soft Labels vom Teacher)
Das Teacher-Modell sagt fÃ¼r "Okay film":

{"Positive": 0.55, "Negative": 0.45}
Das Student-Modell lernt:

"Okay film" ist nicht ganz positiv, sondern eher neutral.
Es bekommt mehr Kontext & Nuancen, was das Lernen verbessert.
4ï¸âƒ£ Wie wird das mathematisch umgesetzt?
Wir kombinieren zwei Loss-Funktionen:

1ï¸âƒ£ KL-Divergenz (Distillation Loss):
Vergleich der Softmax-Wahrscheinlichkeiten vom Teacher & Student.
2ï¸âƒ£ Klassische Cross-Entropy (CE-Loss):
Vergleich mit den echten Labels.
ğŸ“Œ Formel fÃ¼r den kombinierten Loss:

Loss=Î±â‹…KL(softmax(S/T),softmax(T/T))+(1âˆ’Î±)â‹…CE(S,Y)
ğ‘† = Student-Modell-Logits
ğ‘‡ = Teacher-Modell-Logits
ğ‘Œ= Harte Labels
ğ‘‡(Temperatur) â†’ GlÃ¤ttet die Wahrscheinlichkeiten
ğ›¼ â†’ Gewichtung zwischen Distillation und klassischem Loss
ğŸ“Œ Fazit
âœ… Soft Labels enthalten mehr Wissen als harte Labels.
âœ… Das Student-Modell kann besser verallgemeinern & Overfitting vermeiden.
âœ… ErmÃ¶glicht kleinere & schnellere Modelle mit vergleichbarer Performance.
âœ… Wird hÃ¤ufig in Mobile AI & Edge-Deployments genutzt.

ğŸ“Œ Beispiel in der Praxis: Google verwendet Knowledge Distillation fÃ¼r Google Assistant, um ein kleines Modell auf Smartphones laufen zu lassen, das fast so gut ist wie groÃŸe Server-Modelle. ğŸš€